\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{amsmath}

\newcommand{\bigo}{O}
\newcommand{\smallo}{o}
\newcommand{\rank}{\textit{rank}}
\newcommand{\select}{\textit{select}}

\title{Succinct data structures}
\author{Jani Rahkola}

\begin{document}
\maketitle

\section{Introduction}

Succinct data structures store data using near optimal amount of bits
while still enabling efficient operations. The difference to
compressed data structures is that succinct data structures allow
operations without a preceding decompression. Also, the space effiency
of the succinct data structures does not depend on the values of the
input data, as might be the case with a compressed data structure.

Succinct data structures are divided into three groups based on how
close they get to the optimal amount of space. Say $T$ is the optimal
amount of bits needed to store the actual data. The worst of the three
is the class of \emph{compact} data structures where the structure can
take $\bigo(T)$ bits of space.

As a double use of the term, a data structure taking $T + \smallo(T)$
bits is called \emph{succinct}. Classical example of a succinct data
structure is the bit vector with constant time rank and select.

Lastly, a structure taking $T + \bigo(1)$ bits is called
\emph{implicit}. Classical examples of implicit data structures are
the sorted array and the array backed binary heap. Implicit data
structures are basically well chosen permutations of the data that
allow fast queries to be made.

In this report I will use the term \emph{redundancy} to mean the extra
bits needed in addition to the optimal $T$ bits needed by the data
structure. For example, to characterize succinct data structures using
this term is to say that they have $\smallo(T)$ redundancy.

Many of the succinct data structures are static, but some dynamic
variations have been developed. For example, a dynamic variant for the
rank and select bit vector is known.

\section{Warm up}

Lets consider the binary heap as a warm up. We are given $n$ $k$ bit
integers in an array. This takes $nk$ bits and is the optimal space to
store the integers. We can then construct the heap in place in
$\bigo(n)$ time using the following algorithm by Floyd \cite{floyd64}.

Consider the array as an almost complete binary tree. For every
subtree, from the lowest to to the whole tree, move the root of the
subtree down to its correct place. That is, when you heapify the
subtree at level $k$, all the subtrees at level $k-1$ have already
been heapified. Thus it suffices to move the root node to its correct
place. This construction takes $\bigo(n)$ time.

Now to delete the minimum (or maximum, depending on the heap) element,
you move the last element in the array as the new root and decrese the
size of the heap by one. You then move the new root down to its
correct place. As the height of the tree is at most $log(n)$, the time
it takes is $\bigo(log(n))$.

Inserting an element is similar. You add the new node as the last
element in the array and then move it upwards to its correct position.
Once again this takes at most $\bigo(log(n))$ time.

\section{Bit vector with rank and select}

Consider a vector $V$ of bits indexed from $0$ upto $n-1$. For index
$i$, $0 \le i < n$ function $\rank_1(i)$ returns the number of one
bits in subvector $V[0:i+1]$, that is, the sum $\sum_{i=0}^{i} V[i]$.
Say that $V$ has $o$ one bits. Now for $i$, $0 \le i \le o$ the
function $\select_1(i)$ returns the index of the $i$th one bit.

Both problems are easy to solve in linear space, as one can just store
the answers for every index. Jacobson states \cite{jacobson89} that he
solved these problems in his thesis \cite{jacobson88} using
$\smallo(n)$ redundancy, that is, succinctly. His solution allows both
\rank and \select in $\bigo(1)$ time.

Jacobson's solution for \rank first divides the bit vector once and
stores the answers to the query at every point of division. Then every
chunk is divided again into subchunks and an answer relative to the
chunk is stored at every point of subchunk division. By careful choice
of sizes for the chunks and subchunks one can now afford to store a
lookup table for the answers inside a subchunk with redundancy of
still only $\smallo(n)$ bits.

One such choice of sizes is to first dived to chunks of $\log^2(n)$
bits. This means storing $\bigo(n / \log^2(n))$ answers of size at
most $\log(n)$ bits. This takes $\bigo(n / \log(n)) = \smallo(n)$
bits.

We can then further divide into subchunks of $\log(\sqrt{n})$ bits.
This time we need to store $\bigo(n / \log(n))$ answers. But as they
are relative to the chunk, only at most $\log(\log^2(n))$ bits are
needed to store them. This adds up to $\bigo(n\log(\log^2(n)) /
\log(n)) = \smallo(n)$ bits.

Thus we have reduced the problem to a size of $\log(\sqrt{n})$ bits.
This means $2^{\log(\sqrt{n})}$ different answers of size at most
$\log(\log(\sqrt{n}))$ bits. We need to store the answer for every
$\log(\sqrt{n})$ positions so the lookup table takes $\bigo(\sqrt{n}
\log(n) \log(\log(n)))$ bits which is $\smallo(n)$ of redundancy.

Lets call the answers stored at the chunk level $C$, the answers
stored at the subcunk level $SC$ and the lookup table $L$. Now the
following algorithm answers the query $\rank_1(i)$.

\begin{itemize}
\item If $\log^2(n)$ divides $i$
  \begin{itemize}
  \item return $C[i / \log^2(n)]$.
  \end{itemize}

\item If $\log(\sqrt{n})$ divides $i$
  \begin{itemize}
  \item return $C[\lfloor i / \log^2(n)\rfloor ] + SB[i /
    \log(\sqrt{n})]$.
  \end{itemize}

\item Return $C[\lfloor i / \log^2(n)\rfloor ] + SB[\lfloor i /
  \log(\sqrt{n}) \rfloor] + L[i]$.
\end{itemize}

The solution for \select is similar to the solution for \rank. Once
again the bit vector is divided and subdivided into chunks and
subchunks small enough to store a lookup table of the answers in
$\smallo(n)$ bits. But this time we cannot divided into chunks of
equal size. We need to know in which chunk the $i$th one bit is. This
means that the chunks must have equal amount of one bits, but they may
be of different size.

Now if a chunk is sparse, it has relatively few one bits, we can
afford to store all the answers. If the chunk is dense, we need to
subdivide. If a subchunk is spares, we can again just store the
answers. But now also the dense subchunks are small enough, and we can
use a lookup table of $\smallo(n)$ bits.

While succinct with constant time queries, Jacobson's solution has a
redundancy of $\bigo(n \log(\log(n)) / \log(n))$ bits. As Patrascu
explains \cite{patrascu08}, the constant time query locks you in using
a constant amount of subdivisions. In the worst case you have to
branch to the lowest level of subdivision, and thus you can only
afford a constant level of them.

Patrascu was able to overcome this limitations with his recursive
solution \cite{patrascu08} and achieved a redundancy of $\bigo(n /
\log^k(n))$ with queries in $\bigo(k)$ time. That is, you get constant
time queries when you set $k$ to be a constant, but $k$ can be
arbitrary. This allows you to tune the amount of redundancy.

The actual result of Patrascu is a succinct representation of
augmented B-trees. A B-tree is a representation of $B^k = n$ elements
for any $k$, where internal nodes have $B$ children and the elements
are stored in the leafs. You then augment every leaf with a value that
is a function of the element and every internal node with a value that
is a function of the values of the children of the node.

One can solve rank and select using an augmented B-tree
\cite{patrascu08} by storing the bits in the leafs and augmenting
internal nodes with the sum of its children. Using world-level
parallelism one can do the queries in constant time \cite{patrascu08}.

Patrascu shows \cite{patrascu08} that one can store an augmented
B-tree of size $n$ of elements of set $\Sigma$ augmented with values
of set $\Phi$ using $2$ bits and $\bigo(|\Sigma| + |\Phi|^{B+1} +
B|\Phi|^B)$ words of redundancy. Patrascu gives the rank and select
with the redundancy $\bigo(n / \log^k(n))$ as an application of these
succinct augmented B-trees.

\section{Binary trees}

Jacobson uses the rank and select operations to represent a binary
tree using $\smallo(n)$ redundancy \cite{jacobson89}. The supported
operations are \textit{left-child}, \textit{right-child} and
\textit{parent} in $\bigo(1)$ time. Jacobson's solution is a
level-order representation of the tree enconded as follows.

First add marker nodes to the tree so that every original node has two
children. Then travers the tree in level-order, that is from left to
right and top to bottom, and mark into a bit vector a one for every
original node and zero for every marker node. Now the following holds.

\begin{align*}
  \textit{left-child}(i) &= 2\rank_1(i) \\
  \textit{right-child}(i) &= 2\rank_1(i) + 1 \\
  \textit{parent}(i) &= \select_1(\lfloor i / 2 \rfloor )
\end{align*}

Travers a tree in depth-first order and mark an open paren when you
visit a node for the first time, and a closing paren when you leave a
node for the last time. This gives you a representation called
balanced parentheses which can be encoded as a bit vector. Using this
representation Munro and Raman managed to support a one additional
operation, \textit{subtree-size} in constant time with the same
$\smallo(n)$ redundancy \cite{munro01}.

\section{K-ary and other trees}

Suffix trees are an important data structure in string processing that
store all the suffixes of a text in a compressed tree. When
implementing a suffix tree one has to choose a representation for the
child relation, where the children can be labeled with arbitrary
strings. One representation for this relation is a \emph{k-ary tree},
where every node has at most $k$ children, labeled from a set
$\Sigma$. The k-ary tree can store the child relation if we choose
$\Sigma$ to be the alphabet of our string.

Farzan and Munro show \cite{farzan08} how one can store a k-ary tree
with $\smallo(n\log(k))$ redundancy while supporting constant time
\textit{child} operation. This means that for a small alphabet the
child relation takes very little space. Farzan and Munro also show
that all the standard operations for a k-ary tree, for example
\textit{subtree-size} can be supported and that the same technique can
be used to represent what they call \emph{free trees}, trees that have
no order between the children of a node.

\section{Suffix arrays}

Suffix array is an array representation of the suffix tree. In index
$i$ of the array is the starting index of the $i+1$th (if $0 \le i <
n$) suffix of the text in lexicographical order. The advantage of the
array over the tree is the amount of space it requires.
State-of-the-art representations are also \emph{self-indexing}, that
is, they can return the result of any query without the original text.

The first (by Ferragina et al. \cite{ferragina07}) succinct and
self-indexing representation for a suffix array was the
\emph{FM-index} introduced by Ferragina and Manzini
\cite{ferragina05}. For a text $T$ it uses $5nH_k(T) + \smallo(|T|)$
bits where $H_k(T)$ is the k-order entropy of $T$. For a pattern $P$
it takes $\bigo(|P|)$ time to count the number of occurencies and
$\bigo(\log^{1+\epsilon}(|T|))$ time to locate them. As the FM-index
is self-indexing it can also display any substring $S$ of the text and
does it in $\bigo(|S| + \log^{1+\epsilon}(|T|)$ time.

The FM-index consists of three parts: a Burrows-Wheeler transform
(BWT) \cite{burrows94} of the text, a table $C$ which tells for every
character $c$ the number of occurencies in the text of characters
smaller than $c$ and a function $\texttt{Occ}(c,k)$ which tells the
number of occurencies of character $c$ in the prefix of length $k$ of
the BWT. All these are compressed together to achive the space bound
and can be used to answer suffix array queries.

Sadakane introduced \emph{compressed suffix arrays} \cite{sadakane02}
as an different kind of solution to the problem. It was later seen
that they were actually an implementation of the FM-index idea.
Sadakane represents the BWT as a bit vector for every character in the
alphabet. This gives a space bound of $nH_0(T) + \bigo(|T|)$ which
works for any alphabet of size $\bigo(\log(|T|)^{\bigo(1)})$ unlike
the FM-index which assumes a constant alphabet and has an exponential
space dependency to it \cite{ferragina07}.

Ferragina et al. improve \cite{ferragina07} on the previous results on
FM-indices. They combine their generalization of wavelet trees with an
compression technique for the BWT by Ferragina et al.
\cite{ferragina05} and achieve a structure that takes $nH_k(T) +
\bigo(n / \log^{\epsilon}(n))$ bits. Counting occurencies of a pattern
$P$ takes $\bigo(|P|)$ time, locating them
$\bigo(\log^{1+\epsilon}(|T|))$ time and displaying a substring $S$
$\bigo(|S| + \log^{1+\epsilon}(|T|))$ time. The difference to the
original FM-index is that this solution works for any alphabet
$\Sigma$ of size $\log(|T|)^{\bigo(1)}$.

\end{document}
